## Past Reads: February 2023

**Thu, 16th Feb 2023 - Sat, 18th Feb 2023**:

<!-- Reading Sessions

Thu, 16th Feb 2023 - Sat, 18th Feb 2023
A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play
https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphazero-shedding-new-light-on-chess-shogi-and-go/alphazero_preprint.pdf

Conclusions:  The whole paper hinges on the idea of improvement on the PUCT, but as such is just application

Sun, 19th Feb 2023 - Tue, 21th Feb 2023
ABANDONED [Too much math] - Supervised QML is Kernel Methods https://arxiv.org/pdf/2101.11020.pdf
ABANDONED [No model specified] - AlphaFold Paper

Mismatched No More - https://ar5iv.labs.arxiv.org/html/2110.02758

Conclusions - Generalised to attempt to optimise for both end and next step with varying degrees with a parameter alpha visualised from linear interpolation

Wed, 22th Feb 2023
QUBO Formulations for Training Machine Learning Models
https://ar5iv.labs.arxiv.org/html/2008.02369

Conclusions: No conclusion for 66%, i.e pure math. SVMs have better complexity in QUBO as they trade off time complexity to space complexity i.e (O(n3) -> Time: O(Nd) Space: O(N2))

Thu, 23th Feb 2023
Object Representations as Fixed Points: Training Iterative Refinement Algorithms with Implicit Differentiation
https://arxiv.org/pdf/2207.00787.pdf

MAML
https://arxiv.org/pdf/1703.03400.pdf


Conclusions:
- How large a net is can be concluded from Whitney Embedding Theorem such that if Dataset is of m-dims, the manifold generated by the Universal Approximator i.e NN has to be of AT LEAST 2m dims to fully embed the Dataset. This gives us some insight into how large a net should be
- Better algo to generalise against multiple tasks. Looks good makes sense

Fri, 24th Feb 2023
Conditional Neural Processes
https://arxiv.org/pdf/1807.01613.pdf
Neural Complexity Measures
https://papers.nips.cc/paper/2020/file/6e17a5fd135fcaf4b49f2860c2474c7c-Paper.pdf
 -->
