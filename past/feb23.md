## Past Reads: February 2023

### Thu, 16th Feb 2023 - Sat, 18th Feb 2023
<pa-per href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphazero-shedding-new-light-on-chess-shogi-and-go/alphazero_preprint.pdf" title="A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"
tags="RL"></pa-per>

#### Conclusions
The whole paper hinges on the idea of improvement on the PUCT, but as such is just application

### Sun, 19th Feb 2023 - Tue, 21th Feb 2023
> Abandoned [Too much math]

<pa-per href="arxiv.org/pdf/2101.11020.pdf" title="Supervised QML is Kernel Methods" tags="QML"></pa-per>

> Abandoned [No model specified]

<pa-per href="nature.com/articles/s41586-021-03819-2" title="Highly accurate protein structure prediction with AlphaFold" tags="RL"></pa-per>

<pa-per href="ar5iv.labs.arxiv.org/html/2110.02758" title="Mismatched No More: A Unified Framework for Learning to Match and Rank" tags="RL"></pa-per>

#### Conclusions
- Generalised to attempt to optimise for both end and next step with varying degrees with a parameter alpha visualised from linear interpolation

### Wed, 22th Feb 2023
<pa-per href="ar5iv.labs.arxiv.org/html/2008.02369" title="QUBO Formulations for Training Machine Learning Models" tags="QML"></pa-per>

#### Conclusions
- No conclusion for 66%, i.e pure math
- SVMs have better complexity in QUBO as they trade off time complexity to space complexity i.e (O(n3) -> Time: O(Nd) Space: O(N2))

### Thu, 23th Feb 2023
> Abandoned [Too much math]

<pa-per href="arxiv.org/pdf/2207.00787.pdf" title="Object Representations as Fixed Points: Training Iterative Refinement Algorithms with Implicit Differentiation" tags="Meta"></pa-per>

<pa-per href="arxiv.org/pdf/1703.03400.pdf" title="Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks" tags="MeL"></pa-per>

#### Conclusions
- How large a net is can be concluded from Whitney Embedding Theorem such that if Dataset is of m-dims, the manifold generated by the Universal Approximator i.e NN has to be of AT LEAST 2m dims to fully embed the Dataset. This gives us some insight into how large a net should be
- Better algo to generalise against multiple tasks. Looks good makes sense

### Fri, 24th Feb 2023 - Sun, 26th Feb 2023
<pa-per href="arxiv.org/pdf/1807.01613.pdf" title="Conditional Neural Processes" tags="Neuro"></pa-per>

<pa-per href="papers.nips.cc/paper/2020/file/6e17a5fd135fcaf4b49f2860c2474c7c-Paper.pdf" title="Neural Complexity Measures" tags="Neuro"></pa-per>